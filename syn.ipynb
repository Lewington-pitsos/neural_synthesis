{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "First, we download a pre-trained vgg19 model (as recommended in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model onto your GPU (if you have one) and \"freeze\" it, preventing any additional gradient updates from occurring to the already trained model. \n",
    "\n",
    "**Note:** I'm freezing each set of parameters individually *after* loading the model into my GPU. This is because the process of loading into the GPU destroys the old parameter tensors and creates new ones on the GPU (so freezing beforehand would have no effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-c0b19779f170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# stop training the net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg = vgg.to(device)\n",
    "for param in vgg.features.parameters(): # stop training the net\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the network really is loaded onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(vgg.parameters()).is_cuda # Should be `True` if you have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can also replace all the max pool layers in our vgg network with average pool layers as suggested in the paper. Personally I found that this made gradient updates a bit trickier, but it did give nicer results once I got it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_layers = list(vgg.children())[0]\n",
    "\n",
    "for i in range(len(vgg_layers)):\n",
    "    if (vgg_layers[i].__class__.__name__ == \"MaxPool2d\"):\n",
    "        vgg_layers[i] = nn.AvgPool2d(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll remove the fully connected layers from our pretained vgg network. We don't need these for texture synthesis, and leaving them in limits us to input images of size 224x224 only.\n",
    "\n",
    "**Note:** `some_model.children()` returns an iterator containing all of the model's layers. Common practice is to turn this into a list in order to interact with it. With vgg19 what you'll actually get is a list containing two lists. The first of these contains all the layers we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_layers = list(vgg.children())[0]\n",
    "vgg = nn.Sequential(useful_layers)\n",
    "len(list(vgg.children())[0]) # should be 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perpairing the example texture\n",
    "\n",
    "The pre-trained networks from torchvision.models need to be passed inputs in a certain format, so we're going to add a transform to apply those transformations to all the image's we're dealing with. We also define a helper function `image_loader` which takes a relative image path, loads the image into a tensor, applies the relevant transformations and returns it.\n",
    "\n",
    "`show_image` makes it easier to view images we load using `image_loader`. Pytorch doesn't seem to have any good method for visualizing tensors, so we convert the tensor to a numpy array, transpose it and then plot that using matplotlib.\n",
    "\n",
    "**Note:** `unsqueeze()` is a pytorch function that adds extra dimensions to tensors. We're using it here because vgg19 only takes mini-batches of images, and we're working with a single image. We use `.unsqueeze()` to add the extra dimension to our image tensor and trick vgg into thinking it's dealing with a small minibatch of images. \n",
    "\n",
    "**Note 2:** For classification vgg19 expects input images to be normalized according to imagenet mean values. The commented out line below applies this transform. We're not doing classification though and *should not* add it here. It would cause our sample textures to be optimized to look like the *normalized* version of the example texture (i.e. off color and weird). See `data/samples/stones-normalized` for what happened to me. We can also ignore the usual size constraint of 224x224 since convolutional layers are size-agonstic and we already removed all the fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.RandomCrop(250),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def image_loader(image_path):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).float()\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU\n",
    "\n",
    "def show_image(image_tensor):\n",
    "    np_image = image_tensor.squeeze().cpu().detach().numpy()\n",
    "    plt.imshow(np.transpose(np_image, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually load in the image (and also plot it out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/examples/\"\n",
    "img_name = \"rings-2.jpg\" # we use this variable later\n",
    "full_path = img_path + img_name\n",
    "\n",
    "example_texture = image_loader(full_path)\n",
    "show_image(example_texture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Layer activations\n",
    "\n",
    "The guys in the paper defined the texture profile of the example texture as the **gram matrices** of the feature maps of some of the convolutional or pooling layers in vgg19 after feeding in that example texture. In order to get at these feature maps we're going to use pytorch's hook feature. \n",
    "\n",
    "### Hooks\n",
    "\n",
    "Basically hooks are classes with `.hook_fn`, `.__init__` and `close` methods. `__init__` should take a layer from a module and register the hook's `hook_fn` method on that layer. Whenever the layer is run from then on, `hook_fn` for that hook instance will be called, being passed in the model holding the layer, the layer's input and the layer's output in that order. Let's ignore `close()` for now.\n",
    "\n",
    "In this case all we want to do is save the output of each layer, so we create a simple `hook_fn` that just saves the layer's output tensor on the hook instance for later use when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, layer): \n",
    "        self.hook = layer.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): \n",
    "        self.features = output\n",
    "    def close(self): \n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guys from the paper found that we don't need input from every single layer. One from each **scale** of the network (a new scale occurs when the input is down-sampled, i.e. pooling layers) seems to be enough. We register a hook instance on a convolutional or pooling layer from each scale (there are 5 scales in vgg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_feature_maps = []\n",
    "\n",
    "all_layers = list(vgg.children())[0]\n",
    "\n",
    "for index in [2, 5, 18, 25, 36]:\n",
    "    layer = all_layers[index]\n",
    "    layer_name = layer.__class__.__name__\n",
    "    if layer_name == \"Conv2d\"  or layer_name == 'MaxPool2d' or layer_name == 'AvgPool2d':\n",
    "        layer_feature_maps.append(SaveFeatures(layer))\n",
    "\n",
    "print(len(layer_feature_maps)) # should be 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just run our network with all those conv and pooling layers already hooked, and the `.features` attributes of all our hook instances will automatically be populated with the feature maps of all those layers (as per `hook_fn`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(layer_feature_maps[2].features) == type(None)) # should be True the first time you run this\n",
    "vgg(example_texture)\n",
    "print(layer_feature_maps[2].features[0][0][0]) # should be a tensor with random numbers in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have all the feature maps we need, but for texture synthesis we want to make a gram matrix for each layer's feature maps. Let's make a helper function that takes all the feature maps from a layer and returns a gram matrix. I copy-pasted this myself from somewhere random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d) # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have this function, let's use it to create a list of gram matrices, one for each layer we're interested in. This list represents all the summary statistics we will need from the example input. The values in this tensor will probably be tiny, or actually 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stats = [gram_matrix(layer.features) for layer in layer_feature_maps]\n",
    "print(len(example_stats)) # should be 5\n",
    "print(example_stats[0]) # should be a tensor. It will probably have very small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss\n",
    "\n",
    "Next we need a way of generating a loss from the example summary statistics (above), and some sample summary statistics (which we will eventually generate by passing the sample texture through our vgg network in exactly the same way). The paper recommends calculating the **mean squared error** loss between the two sets of gram matrices.\n",
    "\n",
    "I had a lot of trouble with this. The losses were always tiny (probably because, as above, the gram matrices I calculated from the feature maps tended to hold super small values), and often resulted in outright 0 gradients. The hacky-as-balls solution I came up with was to literally just scale the loss by some factor before returning. Gross I know.\n",
    "\n",
    "The paper also recommends using LBFGS to perform gradient updates. `torch.optim` provides a convenience [method](https://pytorch.org/docs/stable/optim.html) for this which you should probably look up. The main takeaway is: if you're using `torch.optim.LBFGS()` as your optimizer, you can't just call `optimizer.step()` like normal. Instead, you need to pass in a special `closure()` function that calculates and returns your loss. Because we can't pass any arguments to the `closure()` function directly, we're going to create a class (`GradientStepper`) around this `closure()` function and use attribute variables on that class as parameters to our `closure()`. \n",
    "\n",
    "We'll also add our loss function to this class for conveniance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientStepper():\n",
    "    def __init__(self, optimizer, model, sample_texture, layer_feature_maps, scale):\n",
    "        self.optimizer = optimizer\n",
    "        self.sample_texture = sample_texture\n",
    "        self.model = model\n",
    "        self.layer_feature_maps = layer_feature_maps\n",
    "        self.scale = scale\n",
    "\n",
    "    def loss_fn(self, stats, target_stats):\n",
    "        loss = 0\n",
    "        for i in range(len(target_stats)):\n",
    "            loss += torch.mean((stats[i] - target_stats[i]).pow(2)) # mean squared error\n",
    "\n",
    "        print(\"scale: {}\".format(self.scale))\n",
    "        return loss * self.scale\n",
    "    \n",
    "    def closure(self):\n",
    "        self.optimizer.zero_grad() # Please read up on this if you don't know what it does. \n",
    "\n",
    "        self.model(self.sample_texture)\n",
    "        sample_stats = [gram_matrix(layer.features) for layer in self.layer_feature_maps]\n",
    "\n",
    "        loss = self.loss_fn(sample_stats, example_stats)\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"loss: {}\".format(loss))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Sample Texture\n",
    "\n",
    "Nearly there. Now we need to create a random image that will eventually become out sample texture. We're going to pass this texture through our vgg network and keep performing gradient descent on it against the above loss function until its summary statistics more or less match the example texture.\n",
    "\n",
    "We use numpy to create the random image here because numpy is generally better at operating on matrices. In particular we want to smoothen the image because real life images have a natural smoothness to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image(blur=6):\n",
    "    # create random noise numpy array\n",
    "    np_sample = np.random.rand(300, 300, 3)\n",
    "\n",
    "    # smooth it out (try commenting out this line and see the difference)\n",
    "    np_sample = scipy.ndimage.filters.median_filter(np_sample, [blur, blur,1]) \n",
    "\n",
    "    # finally convert to a tensor with autograd enabled (since we're \n",
    "    # going to be performing gradient updates on this image)\n",
    "    sample = torch.from_numpy(np_sample).float().permute(2, 0, 1).unsqueeze(0).to(device) \n",
    "    sample.requires_grad = True\n",
    "    \n",
    "    return sample\n",
    "\n",
    "show_image(random_image(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Sample Texture\n",
    "\n",
    "And here we are. Basically all we do now is perform an arbitrary number of gradient updates on the random sample texture. Once that's done it should look a bit like the example texture (hopefully). To make this clean and reusable, we're going to pack this process into a class called `Synthesizer`, which will take in a bunch of hyperparameters and also save the generated textures as `.jpg`'s for viewing later.\n",
    "\n",
    "First though, we're going to define some conveniance functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(name, sample_texture, lr):\n",
    "    if name == \"LBFGS\":\n",
    "        return optim.LBFGS([sample_texture], lr=0.1)\n",
    "    if name == \"Adam\":\n",
    "        return optim.Adam([sample_texture], lr=0.1) \n",
    "    \n",
    "    raise ValueError('Name was not a valid optimizer identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Images\n",
    "\n",
    "Rather than displaying the generated images here in the notebook I'm saving them to directories in this workspace. This way we can examine them at them at our leisure later. In particular I want to save a bunch of images throughout the generation process so I can observe how it's going. I'm going to give each image an \"identifier\", which is basically just a string representation of the hyperparameters used to generate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifier(params):\n",
    "    id = \"\"\n",
    "    for key in params:\n",
    "        if key not in [\"layer_feature_maps\", \"model\", \"iterations\", \"id\"]:\n",
    "            if key == \"name\":\n",
    "                id += params[key].replace(\".jpg\", \"\") + \"-\"\n",
    "            else:\n",
    "                id += \"{}{}-\".format(key, params[key])\n",
    "        \n",
    "    return id[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "The only important method is `synthesize`, the rest of it is just there to save the sample textures to the right directories. Basically this method takes a bunch of hyperparameters, generates a (quasi) random image, and performs gradient updates on it for a given number of iterations using the `GradientStepper` class we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizer():\n",
    "    \n",
    "    sample_directory = \"data/samples\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.save_mode = params[\"save_mode\"]\n",
    "               \n",
    "    def prepair_save_directory(self, params):\n",
    "        self.identifier = params[\"id\"]\n",
    "        \n",
    "        if self.save_mode == \"final\":\n",
    "            self.dir_path = Synthesizer.sample_directory + \"/lol\"\n",
    "            self.save_at = params[\"iterations\"]\n",
    "            \n",
    "        elif self.save_mode == \"throughout\":\n",
    "            self.dir_path = Synthesizer.sample_directory + \"/\" + self.identifier\n",
    "            self.save_at = 1\n",
    "        \n",
    "        if not os.path.exists(self.dir_path):\n",
    "            os.makedirs(self.dir_path)\n",
    "    \n",
    "    def save_path(self):\n",
    "        if self.save_mode == \"final\":\n",
    "            file_name = self.identifier\n",
    "        elif self.save_mode == \"throughout\":\n",
    "            file_name = self.save_at\n",
    "        \n",
    "        return \"{}/{}.jpg\".format(self.dir_path , file_name)\n",
    "    \n",
    "    def synthesize(self, params):\n",
    "        sample_texture = random_image(params[\"blur\"])\n",
    "        optimizer = make_optimizer(params[\"optimizer\"], sample_texture, params[\"lr\"]) \n",
    "        stepper = GradientStepper(optimizer, params[\"model\"], sample_texture, params[\"layer_feature_maps\"], params[\"scale\"])\n",
    "        \n",
    "        self.prepair_save_directory(params)\n",
    "\n",
    "        for i in range (params[\"iterations\"]):\n",
    "            if sample_texture.grad is not None:\n",
    "                print(sample_texture.grad[0][0][0][:4])\n",
    "\n",
    "            optimizer.step(stepper.closure)\n",
    "\n",
    "            # occationally save an image so see how generation is going\n",
    "            if (i + 1) == self.save_at:\n",
    "                save_image(sample_texture, self.save_path())\n",
    "                self.save_at *= 2\n",
    "\n",
    "                if params[\"scale_decay\"] > 0:\n",
    "                    stepper.scale -= (stepper.scale / params[\"scale_decay\"])\n",
    "        \n",
    "        return sample_texture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Feel free to fiddle around with hyperparameters here, but I did a fair bit of fiddling myself, and to be honest  the only thing that seemed to make any real difference was the scale, which just needed to remain > 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rings-2-layers5-blur5-scale_decay0-scale100000-optimizerLBFGS-lr0.1\n",
      "scale: 100000\n",
      "loss: 1.8169599771499634\n",
      "scale: 100000\n",
      "loss: 1.8169561624526978\n",
      "scale: 100000\n",
      "loss: 1.8168998956680298\n",
      "scale: 100000\n",
      "loss: 1.8168442249298096\n",
      "scale: 100000\n",
      "loss: 1.8167877197265625\n",
      "scale: 100000\n",
      "loss: 1.8167314529418945\n",
      "scale: 100000\n",
      "loss: 1.8166753053665161\n",
      "scale: 100000\n",
      "loss: 1.8166192770004272\n",
      "scale: 100000\n",
      "loss: 1.8165630102157593\n",
      "scale: 100000\n",
      "loss: 1.8165069818496704\n",
      "scale: 100000\n",
      "loss: 1.816450834274292\n",
      "scale: 100000\n",
      "loss: 1.8163948059082031\n",
      "scale: 100000\n",
      "loss: 1.8163385391235352\n",
      "scale: 100000\n",
      "loss: 1.8162823915481567\n",
      "scale: 100000\n",
      "loss: 1.8162261247634888\n",
      "scale: 100000\n",
      "loss: 1.8161699771881104\n",
      "scale: 100000\n",
      "loss: 1.8161137104034424\n",
      "scale: 100000\n",
      "loss: 1.8160576820373535\n",
      "scale: 100000\n",
      "loss: 1.8160017728805542\n",
      "scale: 100000\n",
      "loss: 1.8159455060958862\n",
      "tensor([ 4.3928e-06,  4.0520e-06, -8.9131e-06, -8.4364e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 1.8158891201019287\n",
      "scale: 100000\n",
      "loss: 1.815833330154419\n",
      "scale: 100000\n",
      "loss: 1.8157769441604614\n",
      "scale: 100000\n",
      "loss: 1.8157206773757935\n",
      "scale: 100000\n",
      "loss: 1.815664529800415\n",
      "scale: 100000\n",
      "loss: 1.815608263015747\n",
      "scale: 100000\n",
      "loss: 1.8155518770217896\n",
      "scale: 100000\n",
      "loss: 1.8154958486557007\n",
      "scale: 100000\n",
      "loss: 1.8154397010803223\n",
      "scale: 100000\n",
      "loss: 1.8153836727142334\n",
      "scale: 100000\n",
      "loss: 1.8153274059295654\n",
      "scale: 100000\n",
      "loss: 1.815271258354187\n",
      "scale: 100000\n",
      "loss: 1.815214991569519\n",
      "scale: 100000\n",
      "loss: 1.8151588439941406\n",
      "scale: 100000\n",
      "loss: 1.8151025772094727\n",
      "scale: 100000\n",
      "loss: 1.8150464296340942\n",
      "scale: 100000\n",
      "loss: 1.8149900436401367\n",
      "scale: 100000\n",
      "loss: 1.8149341344833374\n",
      "scale: 100000\n",
      "loss: 1.8148777484893799\n",
      "scale: 100000\n",
      "loss: 1.8148216009140015\n",
      "tensor([ 4.3958e-06,  4.0599e-06, -8.9119e-06, -8.4690e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 1.814765214920044\n",
      "scale: 100000\n",
      "loss: 1.814709186553955\n",
      "scale: 100000\n",
      "loss: 1.814652919769287\n",
      "scale: 100000\n",
      "loss: 1.8145965337753296\n",
      "scale: 100000\n",
      "loss: 1.8145403861999512\n",
      "scale: 100000\n",
      "loss: 1.8144841194152832\n",
      "scale: 100000\n",
      "loss: 1.8144279718399048\n",
      "scale: 100000\n",
      "loss: 1.8143715858459473\n",
      "scale: 100000\n",
      "loss: 1.8143155574798584\n",
      "scale: 100000\n",
      "loss: 1.8142591714859009\n",
      "scale: 100000\n",
      "loss: 1.8142027854919434\n",
      "scale: 100000\n",
      "loss: 1.8141465187072754\n",
      "scale: 100000\n",
      "loss: 1.814090371131897\n",
      "scale: 100000\n",
      "loss: 1.814034104347229\n",
      "scale: 100000\n",
      "loss: 1.8139780759811401\n",
      "scale: 100000\n",
      "loss: 1.8139216899871826\n",
      "scale: 100000\n",
      "loss: 1.8138655424118042\n",
      "scale: 100000\n",
      "loss: 1.8138091564178467\n",
      "scale: 100000\n",
      "loss: 1.8137528896331787\n",
      "scale: 100000\n",
      "loss: 1.8136965036392212\n",
      "tensor([ 4.3572e-06,  4.2694e-06, -8.8308e-06, -8.5129e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 1.8136403560638428\n",
      "scale: 100000\n",
      "loss: 1.8135839700698853\n",
      "scale: 100000\n",
      "loss: 1.8135274648666382\n",
      "scale: 100000\n",
      "loss: 1.8134715557098389\n",
      "scale: 100000\n",
      "loss: 1.8134151697158813\n",
      "scale: 100000\n",
      "loss: 1.8133586645126343\n",
      "scale: 100000\n",
      "loss: 1.813302755355835\n",
      "scale: 100000\n",
      "loss: 1.813246488571167\n",
      "scale: 100000\n",
      "loss: 1.8131898641586304\n",
      "scale: 100000\n",
      "loss: 1.813133716583252\n",
      "scale: 100000\n",
      "loss: 1.8130773305892944\n",
      "scale: 100000\n",
      "loss: 1.8130213022232056\n",
      "scale: 100000\n",
      "loss: 1.812964916229248\n",
      "scale: 100000\n",
      "loss: 1.8129085302352905\n",
      "scale: 100000\n",
      "loss: 1.812852144241333\n",
      "scale: 100000\n",
      "loss: 1.812795877456665\n",
      "scale: 100000\n",
      "loss: 1.8127394914627075\n",
      "scale: 100000\n",
      "loss: 1.8126834630966187\n",
      "scale: 100000\n",
      "loss: 1.8126269578933716\n",
      "scale: 100000\n",
      "loss: 1.8125706911087036\n",
      "tensor([ 4.3572e-06,  4.2602e-06, -8.8538e-06, -8.5529e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 1.8125141859054565\n",
      "scale: 100000\n",
      "loss: 1.8124579191207886\n",
      "scale: 100000\n",
      "loss: 1.812401533126831\n",
      "scale: 100000\n",
      "loss: 1.8123453855514526\n",
      "scale: 100000\n",
      "loss: 1.8122891187667847\n",
      "scale: 100000\n",
      "loss: 1.8122326135635376\n",
      "scale: 100000\n",
      "loss: 1.8121763467788696\n",
      "scale: 100000\n",
      "loss: 1.812119960784912\n",
      "scale: 100000\n",
      "loss: 1.8120635747909546\n",
      "scale: 100000\n",
      "loss: 1.812007188796997\n",
      "scale: 100000\n",
      "loss: 1.8119511604309082\n",
      "scale: 100000\n",
      "loss: 1.8118946552276611\n",
      "scale: 100000\n",
      "loss: 1.8118382692337036\n",
      "scale: 100000\n",
      "loss: 1.811781883239746\n",
      "scale: 100000\n",
      "loss: 1.8117254972457886\n",
      "scale: 100000\n",
      "loss: 1.811669111251831\n",
      "scale: 100000\n",
      "loss: 1.8116127252578735\n",
      "scale: 100000\n",
      "loss: 1.811556339263916\n",
      "scale: 100000\n",
      "loss: 1.8114999532699585\n",
      "scale: 100000\n",
      "loss: 1.8114433288574219\n",
      "tensor([ 4.3575e-06,  4.2736e-06, -8.8221e-06, -8.5367e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 1.811387300491333\n",
      "scale: 100000\n",
      "loss: 1.8113306760787964\n",
      "scale: 100000\n",
      "loss: 1.8112742900848389\n",
      "scale: 100000\n",
      "loss: 1.8112179040908813\n",
      "scale: 100000\n",
      "loss: 1.8111613988876343\n",
      "scale: 100000\n",
      "loss: 1.8111051321029663\n",
      "scale: 100000\n",
      "loss: 1.8110487461090088\n",
      "scale: 100000\n",
      "loss: 1.8109923601150513\n",
      "scale: 100000\n",
      "loss: 1.8109359741210938\n",
      "scale: 100000\n",
      "loss: 1.8108795881271362\n",
      "scale: 100000\n",
      "loss: 1.8108232021331787\n",
      "scale: 100000\n",
      "loss: 1.8107666969299316\n",
      "scale: 100000\n",
      "loss: 1.8107104301452637\n",
      "scale: 100000\n",
      "loss: 0.4260094165802002\n",
      "scale: 100000\n",
      "loss: 0.3971409499645233\n",
      "scale: 100000\n",
      "loss: 0.3565372824668884\n",
      "scale: 100000\n",
      "loss: 0.3089158833026886\n",
      "scale: 100000\n",
      "loss: 0.2568248510360718\n",
      "scale: 100000\n",
      "loss: 0.21360497176647186\n",
      "scale: 100000\n",
      "loss: 0.18157972395420074\n",
      "tensor([2.9396e-06, 1.6111e-06, 2.2046e-06, 3.3130e-06], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 0.15810652077198029\n",
      "scale: 100000\n",
      "loss: 0.13958175480365753\n",
      "scale: 100000\n",
      "loss: 0.12492348253726959\n",
      "scale: 100000\n",
      "loss: 0.1130509078502655\n",
      "scale: 100000\n",
      "loss: 0.10294011235237122\n",
      "scale: 100000\n",
      "loss: 0.0937836542725563\n",
      "scale: 100000\n",
      "loss: 0.08537445217370987\n",
      "scale: 100000\n",
      "loss: 0.07724302262067795\n",
      "scale: 100000\n",
      "loss: 0.0693562775850296\n",
      "scale: 100000\n",
      "loss: 0.06185881420969963\n",
      "scale: 100000\n",
      "loss: 0.054754436016082764\n",
      "scale: 100000\n",
      "loss: 0.047977909445762634\n",
      "scale: 100000\n",
      "loss: 0.04164045676589012\n",
      "scale: 100000\n",
      "loss: 0.0359819270670414\n",
      "scale: 100000\n",
      "loss: 0.03135156258940697\n",
      "scale: 100000\n",
      "loss: 0.028086794540286064\n",
      "scale: 100000\n",
      "loss: 0.024021878838539124\n",
      "scale: 100000\n",
      "loss: 0.02034253068268299\n",
      "scale: 100000\n",
      "loss: 0.017528289929032326\n",
      "scale: 100000\n",
      "loss: 0.015136996284127235\n",
      "tensor([ 2.5790e-08, -1.0157e-07,  3.8186e-08,  5.7852e-07], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 0.013404637575149536\n",
      "scale: 100000\n",
      "loss: 0.01187736727297306\n",
      "scale: 100000\n",
      "loss: 0.010649346746504307\n",
      "scale: 100000\n",
      "loss: 0.009566412307322025\n",
      "scale: 100000\n",
      "loss: 0.008719335310161114\n",
      "scale: 100000\n",
      "loss: 0.007940705865621567\n",
      "scale: 100000\n",
      "loss: 0.007274318486452103\n",
      "scale: 100000\n",
      "loss: 0.006674344651401043\n",
      "scale: 100000\n",
      "loss: 0.006222748663276434\n",
      "scale: 100000\n",
      "loss: 0.005807194393128157\n",
      "scale: 100000\n",
      "loss: 0.005406503565609455\n",
      "scale: 100000\n",
      "loss: 0.005085010547190905\n",
      "scale: 100000\n",
      "loss: 0.004783264826983213\n",
      "scale: 100000\n",
      "loss: 0.004482208285480738\n",
      "scale: 100000\n",
      "loss: 0.00421328516677022\n",
      "scale: 100000\n",
      "loss: 0.003948987927287817\n",
      "scale: 100000\n",
      "loss: 0.0037220760714262724\n",
      "scale: 100000\n",
      "loss: 0.0035198063123971224\n",
      "scale: 100000\n",
      "loss: 0.003317380789667368\n",
      "scale: 100000\n",
      "loss: 0.00312622613273561\n",
      "tensor([-5.0348e-08, -8.8366e-08,  1.6449e-09,  3.3057e-08], device='cuda:0')\n",
      "scale: 100000\n",
      "loss: 0.002960299141705036\n",
      "scale: 100000\n",
      "loss: 0.002786597004160285\n",
      "scale: 100000\n",
      "loss: 0.0026388803962618113\n",
      "scale: 100000\n",
      "loss: 0.0025002448819577694\n",
      "scale: 100000\n",
      "loss: 0.002374979667365551\n",
      "scale: 100000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-a7620a9a692c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-13cc2c0387f4>\u001b[0m in \u001b[0;36msynthesize\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_texture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# occationally save an image so see how generation is going\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m# the reason we do this: in a stochastic setting,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mabs_grad_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-0f8c1e0643af>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"name\": img_name,\n",
    "    \"layers\": 5,\n",
    "    \"iterations\": 128,\n",
    "    \"blur\": 5,\n",
    "    \"scale_decay\": 0, # 0 means no scale decay\n",
    "    \"scale\": 100000,\n",
    "    \"model\": vgg,\n",
    "    \"layer_feature_maps\": layer_feature_maps,\n",
    "    \"optimizer\": \"LBFGS\",\n",
    "    \"lr\": 0.1,\n",
    "}\n",
    "\n",
    "meta_params = {\n",
    "    \"save_mode\": \"final\" \n",
    "    # determine whether to save just the final image, or save at intervals while generating\n",
    "}\n",
    "\n",
    "syn = Synthesizer(meta_params)\n",
    "\n",
    "for s in [100000]:\n",
    "    for sd in [0]:\n",
    "        for blur in [6, 4, 3]:\n",
    "            params[\"scale_decay\"] = sd\n",
    "            params[\"scale\"] = s\n",
    "            params[\"blur\"] = blur\n",
    "            params[\"id\"] = identifier(params) \n",
    "\n",
    "            print(params[\"id\"])\n",
    "\n",
    "            syn.synthesize(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
